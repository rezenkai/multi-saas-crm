# Docker Swarm Production Configuration for Analytics Service
# ENTERPRISE-GRADE: High Availability, Load Balancing, Auto-scaling
# Usage: docker stack deploy -c docker-swarm-production.yml analytics-stack

version: '3.8'

services:
  # Analytics Service - Multiple instances with load balancing
  analytics-service:
    image: analytics-service:latest
    ports:
      - "8005:8000"
    environment:
      - NODE_ENV=production
      - PORT=8000
      - DATABASE_HOST=postgres-master
      - DATABASE_PORT=5432
      - DATABASE_NAME=salesforce_clone
      - DATABASE_USER=postgres
      - DATABASE_PASSWORD_FILE=/run/secrets/postgres_password
      - REDIS_HOST=redis-master
      - REDIS_PORT=6379
      - CLICKHOUSE_HOST=clickhouse-shard1
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_DATABASE=crm_analytics
      - CLICKHOUSE_USER=analytics
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
      - JWT_SECRET_FILE=/run/secrets/jwt_secret
      - LOG_LEVEL=INFO
      - LOG_DIR=/app/logs
    secrets:
      - postgres_password
      - clickhouse_password
      - jwt_secret
    networks:
      - analytics-network
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 5s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.id
    volumes:
      - analytics-logs:/app/logs
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8000/health"\]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ML/AI Service - GPU-optimized instances
  ml-ai-service:
    image: ml-ai-service:latest
    ports:
      - "8007:8007"
    environment:
      - CLICKHOUSE_HOST=clickhouse-shard1
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_USER=analytics
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
      - CLICKHOUSE_DATABASE=crm_analytics
      - OPENAI_API_KEY_FILE=/run/secrets/openai_api_key
      - REDIS_HOST=redis-master
      - REDIS_PORT=6379
    secrets:
      - clickhouse_password
      - openai_api_key
    networks:
      - analytics-network
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 120s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      placement:
        constraints:
          - node.role == worker
          - node.labels.compute-type == gpu-enabled
        preferences:
          - spread: node.labels.zone
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8007/health"\]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  # Load Balancer - High Availability Nginx
  nginx-analytics:
    image: nginx:alpine
    ports:
      - target: 80
        published: 8080
        protocol: tcp
        mode: ingress
      - target: 443
        published: 8443
        protocol: tcp
        mode: ingress
    configs:
      - source: nginx_config
        target: /etc/nginx/conf.d/default.conf
    secrets:
      - source: ssl_certificate
        target: /etc/nginx/ssl/server.crt
      - source: ssl_private_key
        target: /etc/nginx/ssl/server.key
    networks:
      - analytics-network
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.2'
          memory: 128M
      placement:
        constraints:
          - node.role == manager
        preferences:
          - spread: node.id
    depends_on:
      - analytics-service
      - ml-ai-service

  # PostgreSQL Master - Primary Database
  postgres-master:
    image: postgres:13
    environment:
      - POSTGRES_DB=salesforce_clone
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD_FILE=/run/secrets/postgres_replication_password
    secrets:
      - postgres_password
      - postgres_replication_password
    volumes:
      - postgres-master-data:/var/lib/postgresql/data
      - ./database/migrations:/docker-entrypoint-initdb.d
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      placement:
        constraints:
          - node.role == manager
          - node.labels.storage-type == ssd
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d salesforce_clone"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQL Read Replica
  postgres-replica:
    image: postgres:13
    environment:
      - PGUSER=postgres
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      - POSTGRES_MASTER_SERVICE=postgres-master
    secrets:
      - postgres_password
      - postgres_replication_password
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      placement:
        constraints:
          - node.labels.storage-type == ssd
    depends_on:
      - postgres-master

  # ClickHouse Cluster - Shard 1
  clickhouse-shard1:
    image: clickhouse/clickhouse-server:23.8-alpine
    environment:
      - CLICKHOUSE_DB=crm_analytics
      - CLICKHOUSE_USER=analytics
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
    secrets:
      - clickhouse_password
    volumes:
      - clickhouse-shard1-data:/var/lib/clickhouse
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.labels.storage-type == ssd
          - node.labels.workload-type == analytics

  # ClickHouse Cluster - Shard 2
  clickhouse-shard2:
    image: clickhouse/clickhouse-server:23.8-alpine
    environment:
      - CLICKHOUSE_DB=crm_analytics
      - CLICKHOUSE_USER=analytics
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
    secrets:
      - clickhouse_password
    volumes:
      - clickhouse-shard2-data:/var/lib/clickhouse
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.labels.storage-type == ssd
          - node.labels.workload-type == analytics

  # Redis Master
  redis-master:
    image: redis:7-alpine
    command: >
      sh -c "
        export REDIS_PASSWORD=$$(cat /run/secrets/redis_password) &&
        exec redis-server --requirepass $$REDIS_PASSWORD --appendonly yes --replica-read-only no
      "
    secrets:
      - redis_password
    volumes:
      - redis-master-data:/data
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
      placement:
        constraints:
          - node.labels.cache-tier == memory-optimized

  # Redis Replicas
  redis-replica:
    image: redis:7-alpine
    command: >
      sh -c "
        export REDIS_PASSWORD=$$(cat /run/secrets/redis_password) &&
        exec redis-server --requirepass $$REDIS_PASSWORD --appendonly yes --replicaof redis-master 6379
      "
    secrets:
      - redis_password
    volumes:
      - redis-replica-data:/data
    networks:
      - analytics-network
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
      placement:
        preferences:
          - spread: node.id
    depends_on:
      - redis-master

  # Grafana - Business Intelligence
  grafana:
    image: grafana/grafana-oss:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_admin_password
      - GF_INSTALL_PLUGINS=clickhouse-datasource
      - GF_SERVER_DOMAIN=${GRAFANA_DOMAIN:-localhost}
      - GF_SERVER_ROOT_URL=https://${GRAFANA_DOMAIN:-localhost}:3001/
    secrets:
      - grafana_admin_password
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      placement:
        constraints:
          - node.labels.workload-type == frontend

  # Apache Superset - Analytics Platform
  superset:
    image: apache/superset:latest
    environment:
      - SUPERSET_SECRET_KEY_FILE=/run/secrets/superset_secret_key
    secrets:
      - superset_secret_key
    volumes:
      - superset-data:/app/superset_home
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      placement:
        constraints:
          - node.labels.workload-type == frontend

  # Prometheus - Monitoring
  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
    volumes:
      - prometheus-data:/prometheus
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      placement:
        constraints:
          - node.role == manager

  # Backup Service
  backup-service:
    image: backup-service:latest
    environment:
      - POSTGRES_HOST=postgres-master
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      - CLICKHOUSE_HOST=clickhouse-shard1
      - CLICKHOUSE_USER=analytics
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
    secrets:
      - postgres_password
      - clickhouse_password
    volumes:
      - backup-data:/app/backups
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - analytics-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 60s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
      placement:
        constraints:
          - node.role == manager

networks:
  analytics-network:
    driver: overlay
    attachable: true
    encrypted: true
    ipam:
      config:
        - subnet: 10.0.9.0/24

volumes:
  postgres-master-data:
    driver: local
  postgres-replica-data:
    driver: local
  clickhouse-shard1-data:
    driver: local
  clickhouse-shard2-data:
    driver: local
  redis-master-data:
    driver: local
  redis-replica-data:
    driver: local
  grafana-data:
    driver: local
  superset-data:
    driver: local
  prometheus-data:
    driver: local
  backup-data:
    driver: local
  analytics-logs:
    driver: local

configs:
  nginx_config:
    file: ./nginx/load-balancer.conf
  prometheus_config:
    file: ./monitoring/prometheus.yml

secrets:
  postgres_password:
    external: true
  postgres_replication_password:
    external: true
  clickhouse_password:
    external: true
  redis_password:
    external: true
  jwt_secret:
    external: true
  grafana_admin_password:
    external: true
  superset_secret_key:
    external: true
  ssl_certificate:
    external: true
  ssl_private_key:
    external: true
  openai_api_key:
    external: true
